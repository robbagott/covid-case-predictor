{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86321275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from joypy import joyplot\n",
    "import seaborn as sb\n",
    "# from sklearn.model_selection import train_test_split \n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27bb6dc8",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cba681",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.read_csv('data/index.csv')\n",
    "subregion_codes = index[(index['country_code'] == 'US') & (index['aggregation_level']==1)]['subregion1_code'].unique()\n",
    "state_indexes = list(map(lambda x: f'US_{x}', list(subregion_codes)))\n",
    "\n",
    "counties = index[index['country_code'] == 'US'].copy()\n",
    "counties['location_key'] = counties['country_code'] + '_' + counties['subregion1_code'] + '_' + counties['subregion2_code']\n",
    "counties = counties[['location_key', 'subregion1_code']]\n",
    "counties = counties.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d44b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology = pd.read_csv('data/epidemiology.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29370ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = pd.read_csv('data/demographics.csv')\n",
    "state_demographics = demographics[demographics['location_key'].isin(state_indexes)]\n",
    "\n",
    "county_demographics = counties.merge(demographics, on='location_key')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c0378ca",
   "metadata": {},
   "source": [
    "## New Confirmed Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_confirmed = epidemiology[['date', 'location_key', 'new_confirmed']]\n",
    "new_confirmed = new_confirmed[new_confirmed['location_key'].isin(state_indexes)]\n",
    "new_confirmed.rename({'new_confirmed':'New Confirmed Cases'},axis=1, inplace=True)\n",
    "new_confirmed['date'] = pd.to_datetime(new_confirmed['date']).dt.date.astype(str)\n",
    "new_confirmed['location_key'] = new_confirmed['location_key'].apply(lambda x: x[3:]) \n",
    "new_confirmed = new_confirmed.sort_values('date')\n",
    "max_value = new_confirmed['New Confirmed Cases'].max()\n",
    "new_confirmed = new_confirmed.drop(index=10148155)\n",
    "new_confirmed\n",
    "\n",
    "new_confirmed_county = epidemiology[['date', 'location_key', 'new_confirmed']]\n",
    "new_confirmed_county = new_confirmed_county.merge(counties, on='location_key')\n",
    "new_confirmed_county = new_confirmed_county[new_confirmed_county['subregion1_code'] == 'RI'] # Limit to RI\n",
    "new_confirmed_county.rename({'new_confirmed':'New Confirmed Cases'},axis=1, inplace=True)\n",
    "new_confirmed_county['date'] = pd.to_datetime(new_confirmed_county['date']).dt.date.astype(str)\n",
    "new_confirmed_county['location_key'] = new_confirmed_county['location_key'].apply(lambda x: x[-5:]) \n",
    "new_confirmed_county = new_confirmed_county.sort_values('date')\n",
    "new_confirmed_county = new_confirmed_county.dropna()\n",
    "print(new_confirmed_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4b6af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(new_confirmed,\n",
    "                    locations='location_key',\n",
    "                    locationmode='USA-states',\n",
    "                    color='New Confirmed Cases',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    scope='usa',\n",
    "                    animation_frame='date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ec98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "with urlopen('https://raw.githubusercontent.com/plotly/datasets/master/geojson-counties-fips.json') as response:\n",
    "    geojson_counties = json.load(response)\n",
    "\n",
    "fig = px.choropleth(new_confirmed_county,\n",
    "                    geojson=geojson_counties,\n",
    "                    locations='location_key',\n",
    "                    locationmode='geojson-id',\n",
    "                    color='New Confirmed Cases',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    scope='usa',\n",
    "                    animation_frame='date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef411fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = state_demographics.loc[:]\n",
    "sd['location_key'] = sd['location_key'].apply(lambda x: x[3:])\n",
    "sd = sd[['location_key', 'population']]\n",
    "\n",
    "merged_cases = new_confirmed.merge(sd, on='location_key')\n",
    "\n",
    "merged_cases['New Percentage Infected'] = merged_cases['New Confirmed Cases'] * 100 / merged_cases['population']\n",
    "merged_cases = merged_cases[['New Percentage Infected', 'location_key', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.choropleth(merged_cases,\n",
    "                    locations='location_key',\n",
    "                    locationmode='USA-states',\n",
    "                    color='New Percentage Infected',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    scope='usa',\n",
    "                    animation_frame='date')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06563e30",
   "metadata": {},
   "source": [
    "### New Confirmed Cases for the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfe5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology[epidemiology['location_key'] == 'US']['new_confirmed'].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e59c3d9",
   "metadata": {},
   "source": [
    "## Cumulative Confirmed Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe53e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_confirmed = epidemiology[['date', 'location_key', 'cumulative_confirmed']]\n",
    "cumulative_confirmed = cumulative_confirmed[cumulative_confirmed['location_key'].isin(state_indexes)]\n",
    "cumulative_confirmed.rename({'cumulative_confirmed':'Cumulative Confirmed Cases'},axis=1, inplace=True)\n",
    "cumulative_confirmed['date'] = pd.to_datetime(cumulative_confirmed['date']).dt.date.astype(str)\n",
    "cumulative_confirmed['location_key'] = cumulative_confirmed['location_key'].apply(lambda x: x[3:]) \n",
    "cumulative_confirmed = cumulative_confirmed.sort_values('date')\n",
    "max_value = cumulative_confirmed['Cumulative Confirmed Cases'].max()\n",
    "cumulative_confirmed = cumulative_confirmed.drop(index=10148155)\n",
    "cumulative_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9779e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(cumulative_confirmed,\n",
    "                    locations='location_key',\n",
    "                    locationmode='USA-states',\n",
    "                    color='Cumulative Confirmed Cases',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    scope='usa',\n",
    "                    animation_frame='date')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2c5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = state_demographics.loc[:]\n",
    "sd['location_key'] = sd['location_key'].apply(lambda x: x[3:])\n",
    "sd = sd[['location_key', 'population']]\n",
    "\n",
    "merged_cases = cumulative_confirmed.merge(sd, on='location_key')\n",
    "\n",
    "merged_cases['Cumulative Percentage Infected'] = merged_cases['Cumulative Confirmed Cases'] * 100 / merged_cases['population']\n",
    "merged_cases = merged_cases[['Cumulative Percentage Infected', 'location_key', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceacb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.choropleth(merged_cases,\n",
    "                    locations='location_key',\n",
    "                    locationmode='USA-states',\n",
    "                    color='Cumulative Percentage Infected',\n",
    "                    color_continuous_scale='Viridis_r',\n",
    "                    scope='usa',\n",
    "                    animation_frame='date')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82aa3def",
   "metadata": {},
   "source": [
    "### New Confirmed Cases for the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c4d19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology[epidemiology['location_key'] == 'US']['cumulative_confirmed'].plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d768138a",
   "metadata": {},
   "source": [
    "## Statewise Comparisons"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0433db9c",
   "metadata": {},
   "source": [
    "### Hospitalizations per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673eaac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalizations = pd.read_csv('data/hospitalizations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitalizations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc25782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories to consider: cumulative_hospitalized_patients, cumulative_intensive_care_patients, cumulative_ventilator_patients at the end of the date range\n",
    "# Normalize these based on the total population for that state\n",
    "state_hospitalizations = hospitalizations[hospitalizations['location_key'].isin(state_indexes)]\n",
    "cumulative_state_hospitalizations = state_hospitalizations[['date', 'location_key', 'cumulative_hospitalized_patients']]\n",
    "final_cumulative_state_hospitalizations = cumulative_state_hospitalizations[cumulative_state_hospitalizations['date'] == '2022-09-15']\n",
    "final_cumulative_state_hospitalizations = final_cumulative_state_hospitalizations.drop('date', axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9eea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_final_cumulative_state_hospitalizations = final_cumulative_state_hospitalizations.set_index('location_key')\n",
    "indexed_state_demographics = state_demographics[['location_key', 'population']].set_index('location_key')\n",
    "joined_df = indexed_final_cumulative_state_hospitalizations.join(indexed_state_demographics)\n",
    "joined_df['percentage'] = joined_df['cumulative_hospitalized_patients']/joined_df['population']*100\n",
    "joined_df = joined_df.sort_values('percentage')\n",
    "joined_df.index = map(lambda x: x[3:], joined_df.index)\n",
    "\n",
    "plt.figure(figsize=(10,12))\n",
    "joined_df['percentage'].plot(kind='barh')\n",
    "plt.xlabel('Hospitalizations as a Percentage of Population (%)')\n",
    "plt.ylabel('State')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e53df",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_hospital_stats = state_hospitalizations[['date', 'location_key', 'current_hospitalized_patients', 'current_intensive_care_patients', 'current_ventilator_patients']]\n",
    "current_hospital_stats = current_hospital_stats.drop('date', axis=1)\n",
    "sum_of_current_hospital_stats = current_hospital_stats.groupby(['location_key']).sum()\n",
    "sum_of_current_hospital_stats = pd.DataFrame(sum_of_current_hospital_stats)\n",
    "indexed_state_demographics = state_demographics[['location_key', 'population']].set_index('location_key')\n",
    "joined_sum_df = sum_of_current_hospital_stats.join(indexed_state_demographics)\n",
    "joined_sum_df['current_hospitalized_patients'] = joined_sum_df['current_hospitalized_patients']/joined_sum_df['population']\n",
    "joined_sum_df['current_intensive_care_patients'] = joined_sum_df['current_intensive_care_patients']/joined_sum_df['population']\n",
    "joined_sum_df['current_ventilator_patients'] = joined_sum_df['current_ventilator_patients']/joined_sum_df['population']\n",
    "joined_sum_df = joined_sum_df.drop('population', axis=1)\n",
    "joined_sum_df = joined_sum_df.div(joined_sum_df.sum(axis=1),axis=0)\n",
    "joined_sum_df = joined_sum_df.sort_values('current_hospitalized_patients')\n",
    "joined_sum_df.index = map(lambda x: x[3:], joined_sum_df.index)\n",
    "\n",
    "plt.figure(figsize=(10,12))\n",
    "ax = plt.axes()\n",
    "joined_sum_df.plot(kind='barh', stacked=True, ax=ax)\n",
    "plt.xlabel('Fraction of Patients with Certain Status')\n",
    "plt.ylabel('State')\n",
    "plt.legend(['Hospitalizations', 'Intensive Care Patients', 'Ventiator Patients'])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34fc17ee",
   "metadata": {},
   "source": [
    "### Age demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from statistics import mean\n",
    "from collections import defaultdict\n",
    "from matplotlib import cm\n",
    "\n",
    "age_columns = ['population_age_00_09', 'population_age_10_19', 'population_age_20_29',\n",
    "               'population_age_30_39', 'population_age_40_49', 'population_age_50_59',\n",
    "               'population_age_60_69', 'population_age_70_79', 'population_age_80_and_older']\n",
    "\n",
    "state_ages = demographics[demographics['location_key'].isin(['US_VT', 'US_WA', 'US_ME', 'US_OR', 'US_KY', 'US_OK', 'US_MT', 'US_FL'])]\n",
    "state_ages = state_ages[['location_key', 'population'] + age_columns]\n",
    "state_ages = state_ages.dropna()\n",
    "state_ages[state_ages.columns.difference(['location_key', 'population'])] = state_ages[state_ages.columns.difference(['location_key', 'population'])].div(state_ages[state_ages.columns.difference(['location_key', 'population'])].sum(axis=1), axis=0)\n",
    "\n",
    "data = defaultdict(list)\n",
    "for index in state_ages.index:\n",
    "    row = state_ages.loc[index].copy()\n",
    "    \n",
    "    counts = defaultdict(int)\n",
    "    total_num = 1000\n",
    "    for _ in range(total_num):\n",
    "        col = row[age_columns].astype(float).idxmax()\n",
    "        row[col] = row[col] - 1/total_num\n",
    "        counts[col] += 1\n",
    "    for column in age_columns:\n",
    "        pattern = r'\\d+'\n",
    "        numbers = [int(match) for match in re.findall(pattern, column)]\n",
    "        avg = math.ceil(mean(numbers)) + 1\n",
    "        data[row['location_key']] += [avg for _ in range(counts[column])] \n",
    "\n",
    "hist_df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "joyplot(hist_df, hist = True, bins = len(age_columns), overlap=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a05ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hist_df['US_FL'],alpha=0.2,label=\"Florida\",bins=len(age_columns)-1,density=True, color=\"blue\")\n",
    "plt.hist(hist_df['US_VT'],alpha=0.2,label=\"Vermont\",bins=len(age_columns)-1,density=True, color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e7dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(hist_df['US_FL'],alpha=0.2,label=\"Florida\",bins=len(age_columns)-1,density=True, color=\"blue\")\n",
    "plt.hist(hist_df['US_KY'],alpha=0.2,label=\"Kentucky\",bins=len(age_columns)-1,density=True, color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69c66603",
   "metadata": {},
   "source": [
    "## Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1a563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4feab6fb",
   "metadata": {},
   "source": [
    "## Vaccine Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b9064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "328315a8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ec09ecd",
   "metadata": {},
   "source": [
    "### Prepare feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98c09e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test features.\n",
    "\n",
    "# Note: density is not included in data. However, population and square acreage (in geo) is available.\n",
    "demographics = pd.read_csv('data/demographics.csv')\n",
    "demographics['population_male'] = demographics['population_male'] / demographics['population'] * 100\n",
    "demographics['population_female'] = demographics['population_female'] / demographics['population'] * 100\n",
    "demographics['population_age_30_39'] = demographics['population_age_30_39'] / demographics['population'] * 100\n",
    "demographics['population_age_40_49'] = demographics['population_age_40_49'] / demographics['population'] * 100\n",
    "demographics['population_age_50_59'] = demographics['population_age_50_59'] / demographics['population'] * 100\n",
    "demographics['population_age_60_69'] = demographics['population_age_60_69'] / demographics['population'] * 100\n",
    "demographics['population_age_70_79'] = demographics['population_age_70_79'] / demographics['population'] * 100\n",
    "demographics['population_age_80_and_older'] = demographics['population_age_80_and_older'] / demographics['population'] * 100\n",
    "county_data = counties.merge(demographics[[\n",
    "    'location_key',\n",
    "    'population',\n",
    "    'population_male',\n",
    "    'population_female',\n",
    "    'population_age_30_39',\n",
    "    'population_age_40_49',\n",
    "    'population_age_50_59',\n",
    "    'population_age_60_69',\n",
    "    'population_age_70_79',\n",
    "    'population_age_80_and_older',\n",
    "]], on='location_key')\n",
    "\n",
    "# Geographic data\n",
    "geography = pd.read_csv('data/geography.csv')\n",
    "county_data = county_data.merge(geography[[\n",
    "    'location_key',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'area_sq_km',\n",
    "    'elevation_m'\n",
    "]], on='location_key') # Elevation is NaN fairly often\n",
    "\n",
    "# Remove population and sq_km in favor of density\n",
    "county_data['pop_per_km'] = county_data['population'] / county_data['area_sq_km']\n",
    "county_data.drop(columns=['area_sq_km', 'population'], inplace=True)\n",
    "\n",
    "# Economic data from census\n",
    "census_economic = pd.read_csv('data/census_economy.csv', encoding_errors='ignore')\n",
    "census_economic.rename(columns={\n",
    "    'Geography': 'fips',\n",
    "    'Estimate!!Occupied housing units!!Occupied housing units!!HOUSEHOLD INCOME IN THE PAST 12 MONTHS (IN 2021 INFLATION-ADJUSTED DOLLARS)!!Median household income (dollars)': 'median_income',\n",
    "    'Estimate!!Occupied housing units!!Occupied housing units!!MONTHLY HOUSING COSTS!!Median (dollars)': 'median_housing_cost'\n",
    "    }, inplace=True)\n",
    "census_economic = census_economic[[\n",
    "    'fips', \n",
    "    'median_income',\n",
    "    'median_housing_cost'\n",
    "    ]]\n",
    "census_economic['fips'] = census_economic['fips'].str.slice(start=-5)\n",
    "county_data['fips'] = county_data['location_key'].str.slice(start=-5)\n",
    "county_data = county_data.merge(census_economic[['fips', 'median_income', 'median_housing_cost']], on='fips')\n",
    "\n",
    "# Insurance data from census\n",
    "census_insured = pd.read_csv('data/census_insured.csv', encoding_errors='ignore')\n",
    "census_insured.rename(columns={\n",
    "    'Geography': 'fips',\n",
    "    'Estimate!!Percent Insured!!Civilian noninstitutionalized population': 'percent_insured'\n",
    "}, inplace=True)\n",
    "census_insured['fips'] = census_economic['fips'].str.slice(start=-5)\n",
    "county_data = county_data.merge(census_insured[['fips', 'percent_insured']], on='fips')\n",
    "\n",
    "# Demographics data from census.\n",
    "census_demographics= pd.read_csv('data/census_demographics.csv', encoding_errors='ignore')\n",
    "census_demographics.rename(columns={\n",
    "    'Geography': 'fips',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!White': 'white',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!Black or African American': 'black',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native': 'native',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!Asian': 'asian',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander': 'pacific',\n",
    "    '2021 Estimate!!RACE!!Total population!!One race!!Some other race': 'other',\n",
    "    '2021 Estimate!!RACE!!Total population!!Two or more races!!White and Black or African American': 'white and black',\n",
    "    '2021 Estimate!!RACE!!Total population!!Two or more races!!White and American Indian and Alaska Native': 'white and native',\n",
    "    '2021 Estimate!!RACE!!Total population!!Two or more races!!White and Asian': 'white and asian',\n",
    "    '2021 Estimate!!RACE!!Total population!!Two or more races!!Black or African American and American Indian and Alaska Native': 'black and native',\n",
    "}, inplace=True)\n",
    "census_demographics['fips'] = census_demographics['fips'].str.slice(start=-5)\n",
    "county_data = county_data.merge(census_demographics[[\n",
    "    'fips', \n",
    "    'white',\n",
    "    'black',\n",
    "    'native',\n",
    "    'asian',\n",
    "    'pacific',\n",
    "    'other',\n",
    "    'white and black',\n",
    "    'white and native',\n",
    "    'white and asian',\n",
    "    'black and native',\n",
    "    ]], on='fips')\n",
    "\n",
    "# TODO: Vaccinations is time data\n",
    "vaccinations = pd.read_csv('data/vaccinations.csv')\n",
    "\n",
    "print(county_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing elevation values from state values.\n",
    "state_elevation = geography[geography['location_key'].isin(state_indexes)][['location_key', 'elevation_m']]\n",
    "state_elevation['location_key'] = state_elevation['location_key'].apply(lambda x: x[3:])\n",
    "\n",
    "def alter_row(row):\n",
    "    if np.isnan(row['elevation_m']):\n",
    "        subregion = row['subregion1_code']\n",
    "        elevation = state_elevation[state_elevation['location_key'] == subregion]['elevation_m'].item()\n",
    "        return elevation\n",
    "    else:\n",
    "        return row['elevation_m']\n",
    "\n",
    "county_data['elevation_m'] = county_data.apply(lambda row: alter_row(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns for experimentation purposes.\n",
    "county_data = county_data.drop(columns=['population_female'])\n",
    "county_data = county_data.drop(columns=['population_male'])\n",
    "county_data = county_data.drop(columns=['elevation_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe9bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for column in ['white', 'black', 'native', 'asian', 'pacific', 'other', 'white and black', 'white and native', 'white and asian', 'black and native']:\n",
    "    county_data[column] = pd.to_numeric(county_data[column], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a971c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "perc_missing_per_ftr = county_data.isnull().sum(axis=0)/county_data.shape[0]\n",
    "print('fraction of missing values in features:')\n",
    "print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "frac_missing = sum(county_data.isnull().sum(axis=1)!=0)/county_data.shape[0]\n",
    "print('fraction of points with missing values:',frac_missing)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77af7c3b",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2657ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "epidemiology = pd.read_csv('data/epidemiology.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80b0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_surge_data(date_begin, date_end):\n",
    "    county_demographics = demographics[\n",
    "        demographics['location_key'].isin(counties['location_key'].tolist())\n",
    "    ][['location_key', 'population']]\n",
    "\n",
    "    surge_begin = epidemiology[\n",
    "        (epidemiology['location_key'].isin(counties['location_key'].tolist())) & \n",
    "        (epidemiology['date'] == date_begin)\n",
    "    ][['location_key', 'cumulative_confirmed']]\n",
    "    surge_begin.rename(columns={'cumulative_confirmed': 'cum_begin'}, inplace=True)\n",
    "\n",
    "    surge_end = epidemiology[\n",
    "        (epidemiology['location_key'].isin(counties['location_key'].tolist())) & \n",
    "        (epidemiology['date'] == date_end)\n",
    "    ][['location_key', 'cumulative_confirmed']]\n",
    "    surge_end.rename(columns={'cumulative_confirmed': 'cum_end'}, inplace=True)\n",
    "\n",
    "    surge = surge_begin.merge(surge_end, on='location_key')\n",
    "    surge['cumulative_confirmed'] = surge['cum_end'] - surge['cum_begin']\n",
    "\n",
    "    merged = county_demographics.merge(surge, on='location_key')\n",
    "    merged['cumulative_confirmed'] = merged['cumulative_confirmed'] / merged['population']\n",
    "    merged.drop(columns=['population', 'cum_begin', 'cum_end'], inplace=True)\n",
    "    return merged\n",
    "\n",
    "surge_1 = gen_surge_data('2020-10-01', '2021-03-01')\n",
    "surge_2 = gen_surge_data('2021-07-21', '2021-11-01')\n",
    "surge_3 = gen_surge_data('2021-11-21', '2022-03-12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028113b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_1['cumulative_confirmed'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b5284",
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_2['cumulative_confirmed'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c189c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "surge_3['cumulative_confirmed'].hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_stats(data):\n",
    "    perc_missing_per_ftr = data.isnull().sum(axis=0)/data.shape[0]\n",
    "    print('fraction of missing values in features:')\n",
    "    print(perc_missing_per_ftr[perc_missing_per_ftr > 0])\n",
    "    frac_missing = sum(data.isnull().sum(axis=1)!=0)/data.shape[0]\n",
    "    print('fraction of points with missing values:',frac_missing)\n",
    "missing_stats(surge_1)\n",
    "missing_stats(surge_2)\n",
    "missing_stats(surge_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa6af7ed",
   "metadata": {},
   "source": [
    "## Merge and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_clean(data):\n",
    "    data = county_data.merge(data, on='location_key')\n",
    "    data = data.drop(columns=['fips', 'location_key', 'subregion1_code'])\n",
    "    return data\n",
    "surge_1 = merge_and_clean(surge_1)\n",
    "surge_2 = merge_and_clean(surge_2)\n",
    "surge_3 = merge_and_clean(surge_3)\n",
    "surge_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d41377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_heatmap(data):\n",
    "    corr = data.drop(columns=['cumulative_confirmed']).corr()\n",
    "    f, ax = plt.subplots(figsize=(13, 11))\n",
    "    dataplot = sb.heatmap(corr, cmap=\"YlGnBu\", annot=True, vmin=-1, vmax=1)\n",
    "    plt.show()\n",
    "show_heatmap(surge_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalize_data(data):\n",
    "    X = data.drop(columns=['cumulative_confirmed'])\n",
    "    y = data['cumulative_confirmed']\n",
    "    feature_names = X.columns.tolist()\n",
    "    return X, y, feature_names\n",
    "\n",
    "X1, y1, feature_names_1 = finalize_data(surge_1)\n",
    "X2, y2, feature_names_2 = finalize_data(surge_2)\n",
    "X3, y3, feature_names_3 = finalize_data(surge_3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e12ac2cc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f122e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "def ML_pipeline(X, y, param_grid, fit_function, random_state):\n",
    "    # first split to separate out the training set\n",
    "    X_train, X_other, y_train, y_other = train_test_split(X, y, train_size = 0.9, random_state=random_state)\n",
    "\n",
    "    # second split to separate out the validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, train_size=0.5, random_state=random_state)\n",
    "\n",
    "    # Preprocess features\n",
    "    std_ftrs = X.columns.tolist()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "    X_train_prep = pipeline.fit_transform(X_train)\n",
    "    X_val_prep = pipeline.transform(X_val)\n",
    "    X_test_prep = pipeline.transform(X_test)\n",
    "\n",
    "    # we save the train and validation scores\n",
    "    # the validation scores are necessary to select the best model\n",
    "    train_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    val_score = np.zeros(len(ParameterGrid(param_grid)))\n",
    "    models = []\n",
    "\n",
    "    # loop through all combinations of hyperparameter combos\n",
    "    for p in range(len(ParameterGrid(param_grid))):\n",
    "        params = ParameterGrid(param_grid)[p]\n",
    "        print(f\"\\tCurrent parameters: {params}\")\n",
    "        eval_set = [(X_val_prep, y_val)]\n",
    "        \n",
    "        fit_model = fit_function(X_train_prep, y_train, eval_set, params)\n",
    "        models.append(fit_model) # save it\n",
    "        \n",
    "        # calculate train and validation accuracy scores\n",
    "        y_train_pred = fit_model.predict(X_train_prep)\n",
    "        train_score[p] = mean_squared_error(y_train, y_train_pred)\n",
    "        y_val_pred = fit_model.predict(X_val_prep)\n",
    "        val_score[p] = mean_squared_error(y_val, y_val_pred)\n",
    "        print('   ',train_score[p], val_score[p])\n",
    "\n",
    "    print('best model parameters:',ParameterGrid(param_grid)[np.argmax(val_score)])\n",
    "    print('corresponding validation score:',np.max(val_score))\n",
    "    # collect and save the best model\n",
    "    final_model = models[np.argmax(val_score)]\n",
    "    # calculate and save the test score\n",
    "    y_test_pred = final_model.predict(X_test_prep)\n",
    "    test_score = mean_squared_error(y_test,y_test_pred)\n",
    "    print('test score:',test_score)\n",
    "    return final_model, test_score, X_test_prep, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e67b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost\n",
    "\n",
    "def XGB_fit_function(X_train_prep, y_train, eval_set, params):    \n",
    "    XGB = xgboost.XGBRegressor(n_jobs=-1, verbosity = 0)\n",
    "    XGB.set_params(**params)\n",
    "    XGB.fit(X_train_prep, y_train, eval_set=eval_set, verbose=False) \n",
    "    return XGB\n",
    "\n",
    "def gen_best_model(X, y):\n",
    "    num_random_states = 3\n",
    "\n",
    "    XGB_test_scores = []\n",
    "    XGB_models = []\n",
    "    XGB_X_test_preps = []\n",
    "    XGB_y_tests = []\n",
    "\n",
    "    for i in range(num_random_states):\n",
    "        print(f'Random state: {i}')\n",
    "        random_state = i * 42\n",
    "        XGB_param_grid = {\n",
    "            \"learning_rate\": [0.03],\n",
    "            \"n_estimators\": [100],\n",
    "            \"seed\": [random_state], \n",
    "            \"max_depth\": [1,3,10],\n",
    "            \"colsample_bytree\": [0.9],              \n",
    "            \"subsample\": [0.66],\n",
    "            \"eval_metric\": [mean_squared_error],\n",
    "        }\n",
    "        XGB_model, XGB_test_score, XGB_X_test_prep, XGB_y_test = ML_pipeline(X, y, XGB_param_grid, XGB_fit_function, random_state)\n",
    "        \n",
    "        XGB_models.append(XGB_model)\n",
    "        XGB_test_scores.append(XGB_test_score)\n",
    "        XGB_X_test_preps.append(XGB_X_test_prep)\n",
    "        XGB_y_tests.append(XGB_y_test)\n",
    "\n",
    "    print(f'Mean of test scores: {np.mean(XGB_test_scores)}')\n",
    "    print(f'STD of test scores: {np.std(XGB_test_scores)}')\n",
    "\n",
    "    best_index = np.argmax(XGB_test_scores)\n",
    "    best_model = XGB_models[best_index]\n",
    "    return best_model\n",
    "best_model_1 = gen_best_model(X1, y1)\n",
    "best_model_2 = gen_best_model(X2, y2)\n",
    "best_model_3 = gen_best_model(X3, y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f9c2cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def show_feature_importance(model, feature_names):\n",
    "    for type in [\"weight\", \"gain\", \"cover\", \"total_cover\", \"total_gain\"]:\n",
    "        importances = model.get_booster().get_score(importance_type=type)\n",
    "        print(len(importances))\n",
    "        importances_keys = np.array(list(importances.keys()))\n",
    "        importances_values = np.array(list(importances.values()))\n",
    "        print(len(importances_values))\n",
    "        sorted_indcs = np.argsort(importances_values)[::-1][:10][::-1]\n",
    "        print(len(sorted_indcs))\n",
    "\n",
    "        print(importances_values[sorted_indcs].shape)\n",
    "        plt.barh(list(range(10)), importances_values[sorted_indcs], align='center')\n",
    "        plt.yticks(list(range(10)), np.array(feature_names)[sorted_indcs])\n",
    "        plt.ylabel(\"Features\")\n",
    "        plt.xlabel(\"Feature importance\")\n",
    "        plt.title(f\"XGBoost Feature Importance: {type}\")\n",
    "    #     plt.savefig(f'../figures/{tag}_xgboost_feature_importance_{type}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d511de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00808f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importance(best_model_1, feature_names_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1659c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importance(best_model_2, feature_names_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb7e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_feature_importance(best_model_3, feature_names_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70a3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.inspection import permutation_importance\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import roc_curve\n",
    "# import shap\n",
    "\n",
    "# def understand_results(tag, model, test_score, X_test_prep, y_test, feature_names, \n",
    "#                        confusion_matrix=True, \n",
    "#                        roc_curve=False,\n",
    "#                        permutation_feature_importance=False,\n",
    "#                        xgboost_importances=False,\n",
    "#                        shap_importance=False,\n",
    "#                        shap_tree_importance=False):\n",
    "#     if confusion_matrix:\n",
    "#         disp = ConfusionMatrixDisplay.from_estimator(model, X_test_prep, y_test, display_labels=['Cancelled','LargeDelay','MediumDelay','OnTime','SlightDelay'], xticks_rotation='vertical')\n",
    "#         disp.plot()\n",
    "#         plt.title(f\"{tag} Overall Confusion Matrix\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.xticks(rotation='vertical')\n",
    "#         plt.savefig(f'../figures/{tag}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "        \n",
    "#     if roc_curve:\n",
    "#         fpr, tpr, p_crits = roc_curve(y_test, model.predict_proba(X_test_prep))\n",
    "#         plt.plot(fpr,tpr)\n",
    "#         plt.xlabel('fpr')\n",
    "#         plt.ylabel('tpr')\n",
    "#         plt.title(f'{tag} ROC curve')\n",
    "#         plt.show()\n",
    "\n",
    "#     if permutation_feature_importance:\n",
    "#         y_pred = model.predict(X_test_prep)\n",
    "#         result = permutation_importance(model, X_test_prep, y_test, n_repeats=10, random_state=0)\n",
    "#         sorted_indcs = np.argsort(result.importances_mean)[::-1][:10][::-1]\n",
    "#         plt.rcParams.update({'font.size': 13})\n",
    "#         plt.boxplot(test_score-result.importances[sorted_indcs].T, labels=feature_names[sorted_indcs], vert=False)\n",
    "#         plt.axvline(test_score, label='test score')\n",
    "#         plt.title(f\"{tag} Permutation Importances (test set)\")\n",
    "#         plt.xlabel('Score with perturbed feature')\n",
    "#         plt.ylabel(\"Features\")\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.savefig(f'../figures/{tag}_permutation_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "    \n",
    "#     if xgboost_importances:\n",
    "#         for type in [\"weight\", \"gain\", \"cover\", \"total_cover\", \"total_gain\"]:\n",
    "#             importances = model.get_booster().get_score(importance_type=type)\n",
    "#             importances_keys = np.array(list(importances.keys()))\n",
    "#             importances_values = np.array(list(importances.values()))\n",
    "#             sorted_indcs = np.argsort(importances_values)[::-1][:10][::-1]\n",
    "\n",
    "#             plt.barh(list(range(10)), importances_values[sorted_indcs], align='center')\n",
    "#             plt.yticks(list(range(10)), feature_names[sorted_indcs])\n",
    "#             plt.ylabel(\"Features\")\n",
    "#             plt.xlabel(\"Feature importance\")\n",
    "#             plt.title(f\"XGBoost Feature Importance: {type}\")\n",
    "#             plt.savefig(f'../figures/{tag}_xgboost_feature_importance_{type}.png', dpi=300, bbox_inches='tight')\n",
    "#             plt.show()\n",
    "            \n",
    "#     if shap_importance:\n",
    "#         shap.initjs()\n",
    "#         explainer = shap.KernelExplainer(model.predict_proba, X_test_prep)\n",
    "#         shap_values = explainer.shap_values(X_test_prep)\n",
    "#         shap.bar_plot(np.mean(np.abs(shap_values[0]), axis=0), feature_names=feature_names, max_display=10, show=False)\n",
    "#         plt.title(f'{tag} SHAP Global Feature Importance')\n",
    "#         plt.savefig(f'../figures/{tag}_shap_global.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "\n",
    "#         index = 0\n",
    "#         shap.force_plot(explainer.expected_value[0], shap_values[0][index,:], features = X_test_prep[index, :], feature_names = feature_names, show=False, matplotlib=True)\n",
    "#         plt.title(f'{tag} SHAP Local Feature Importance')\n",
    "#         plt.savefig(f'../figures/{tag}_shap_local.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "\n",
    "#     if shap_tree_importance:\n",
    "#         shap.initjs()\n",
    "#         explainer = shap.TreeExplainer(model)\n",
    "#         shap_values = explainer.shap_values(X_test_prep)\n",
    "#         shap.bar_plot(np.mean(np.abs(shap_values[0]), axis=0), feature_names=feature_names, max_display=10, show=False)\n",
    "#         plt.title(f'{tag} SHAP Global Feature Importance')\n",
    "#         plt.savefig(f'../figures/{tag}_shap_global.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "        \n",
    "#         index = 0\n",
    "#         shap.force_plot(explainer.expected_value[0], shap_values[0][index,:], features = X_test_prep[index, :], feature_names = feature_names, show=False, matplotlib=True)\n",
    "#         plt.title(f'{tag} SHAP Local Feature Importance')\n",
    "#         plt.savefig(f'../figures/{tag}_shap_local.png', dpi=300, bbox_inches='tight')\n",
    "#         plt.show()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
